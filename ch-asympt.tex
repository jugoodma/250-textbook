\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Asymptotic Analysis}

\epigraph{In algorithms, as in life, persistence usually pays off.}{Steven S.\ Skiena}

\minitoc

\section{Introduction}

What is an algorithm?

\begin{defn}[Algorithm]
	A recipe/description of a process that accomplishes a goal
\end{defn}

Why study algorithms? First, you will gain tools to \textit{compare} algorithms. Second, studying algorithms will help you to creatively make \textit{new} algorithms.

Why study algorithms in discrete math? Computers are discrete. Computers run programs, which are based on algorithms. Hence, algorithms must be discrete.

\section{\(\Oh\), \(\Omega\), and \(\Theta\) Notations}

Our motivation here is to describe the \textit{growth-rate} of functions. The growth-rate class gives us key insights to what the actual function does on large inputs.

\begin{defn}[Big-Oh]
	An upper-bound on the growth-rate of a function. Formally: \[f(n) \in \Oh(g(n)) \Leftrightarrow (\exists n_0 \in \N, \exists c \in \R^+, \forall n \in \N^{\geq n_0})[f(n) \leq c \cdot g(n)]\]
\end{defn}

\begin{defn}[Big-Omega]
	A lower-bound on the growth-rate of a function. Formally: \[f(n) \in \Omega(g(n)) \Leftrightarrow (\exists n_0 \in \N, \exists c \in \R^+, \forall n \in \N^{\geq n_0})[f(n) \geq c \cdot g(n)]\]
\end{defn}

\begin{defn}[Big-Theta]
	An exact growth-rate of a function. Formally: \[f(n) \in \Theta(g(n)) \Leftrightarrow f(n) \in \Oh(g(n)) \land f(n) \in \Omega(g(n))\]
\end{defn}

Informally, \(f(n) \in \Oh(g(n))\) just means that \textbf{eventually} (for big enough \(n\)) we have that (some constant-multiple of) \(g\) overtakes \(f\).
For example, \(2^n \in \Oh(n!)\) since \(2^n < n!\) (\(n!\) overtakes \(2^n\)) for \(n \geq 4\) (\(n \in \N\)).
So the growth-rate of \(2^n\) is bounded \textit{above} by \(n!\).
Similarly, \(n! \in \Omega(2^n)\) since \(n! > 2^n\) for \(n \geq 4\).
So \(n!\) is bounded \textit{below} by \(2^n\).
Oftentimes in algorithm analysis we aim to show an algorithm has exact run-time (\(\Theta\)), and we do this by showing it has both \(\Oh\) and \(\Omega\) run-time.
Sometimes, though, this is hard to do.

\begin{rem}
	You may see \(\Oh,\Omega,\Theta\) defined using \(\forall n \in \N^{> n_0}\).
	It does not actually matter.
	Just consult whichever definition your instructor uses.
\end{rem}

\begin{rem}
	You may see \(f(n) = \Oh(g(n))\) instead of \(f(n) \in \Oh(g(n))\).
	Both notations represent the same thing.
\end{rem}

\begin{rem}
	We need not use \(n\) for our function variable.
\end{rem}

\exproof{
	Show that \(f(x) = x^2 + 2x - 4 = \Oh(x^2)\).
}{
	To show \(\Oh\), it suffices to find a constant \(c\) and starting point \(x_0\).
	A typical way to do this is by selecting a starting point that might help us down the line.
	Consider \(x_0 = 1\).
	Then, \(x \geq x_0 = 1\).
	Then \(x \leq x^2\) so \(2x \leq 2x^2\) (this is where our starting point choice helps us).
	Thus \(x^2 + 2x - 4 \leq x^2 + 2x \leq x^2 + 2x^2 = 3x^2\).
	So we let \(c = 3\) and we are done.
}

\exproof{
	Show that \(f(x) = x^2 + 2x - 4 = \Omega(x^2)\).
}{
	We take a similar approach as in the previous example.
	Let \(x_0 = 4\).
	Then \(x \geq x_0 = 4\).
	Thus \(x \geq 4 = \frac{2}{1-\frac{1}{2}}\) so \((1-\frac{1}{2}x \geq 2\) so \(x - \frac{1}{2}x \geq 2\) thus \(x - 2 \geq \frac{1}{2}x\).
	Since \(x+2 \geq x-2\) then \(x+2 \geq \frac{1}{2}x\).
	Thus \((x+2)(x-2) \geq (\frac{1}{2}x)^2 = \frac{1}{4}x^2\).
	Since \((x+2)(x-2) = x^2 - 4\) and \(x \geq 4 > 0\) so \(2x > 0\), then \(x^2+2x-4 \geq x^2-4 \geq \frac{1}{4}x^2\).
	So we let \(c = \frac{1}{4}\) and we are done.
}

\begin{rem}
	We can conclude that our \(f(x)\) defined above is in \(\Theta(x^2)\) since it satisfies both \(\Oh\) and \(\Omega\).
	Note that the \(c,x_0\) used in the \(\Oh\) proof need not be the same as those in the \(\Omega\) proof (and vice versa).
	They only need to exist.
\end{rem}

A picture, to see what is going on.

%\pgfmathdeclarefunction{bound}{1}{%
%	\pgfmathparse{#1*x^2}%
%}
\begin{boxx}
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				mark=none,smooth,
				axis x line=bottom, % no box around the plot, only x and y axis
				axis y line=left,
				every fill between plot/.append style={fill=gray,fill opacity=0.1},
				samples=50]
				\addplot [name path=up,color=red,domain=4:6.5] {(3)*x^2} node[pos=1] (up) {};
				\addplot [name path=lo,color=blue,domain=4:6.5] {(1/4)*x^2} node[pos=1] (lo) {};
				\addplot [gray] fill between[of=up and lo];
				\addplot [color=black,domain=0:6.5] {x^2+2*x-4} node[pos=1] (func) {};
				\addplot [color=red,domain=0:6.5] {(3)*x^2} node[pos=1] (upper) {};
				\addplot [color=blue,domain=0:6.5] {(1/4)*x^2} node[pos=1] (lower) {};
				\node [label=135:{\(x_0\)},circle,fill,inner sep=1pt] at (axis cs:1,3) {};
				\node [label=135:{\(x_0\)},circle,fill,inner sep=1pt] at (axis cs:4,4) {};
			\end{axis}
			\node [above, color=black] at (func) {\(f(x)\)};
			\node [above, color=red] at (upper) {\(\Oh\)};
			\node [above, color=blue] at (lower) {\(\Omega\)};
		\end{tikzpicture}
	\end{center}
\end{boxx}

It should be easy to see from this picture that we can make tighter bounds by changing \(c\).
If you change your \(c\), then your \(x_0\) usually will also change.
It should also be easy to see from this picture that for a given \(c\), we may be able to find better \(x_0\).
In this case, we could move our \(\Omega\) \(x_0\) closer to 2.
Note that our function \(f\) is still bounded by our \(\Oh\) and \(\Omega\) functions.
As long as \(c,x_0\) \textit{exist}, we are good to go.

It might be confusing seeing these proofs, as we somewhat made everything appear out of nowhere.
Often, we solve these proofs using scratch work, then cleaning it up for presentation.

\begin{example}
	For \(\Omega\), we started at the goal of \(x^2 + 2x - 4 \stackrel{?}{\geq} cx^2\).
	Note that \(x^2 + 2x - 4 \geq x^2-4 = (x+2)(x-2)\).
	Since \(x+2 > x-2\) then if we can find \(d > 0\) such that \(x-2 \geq dx\) then we can conclude that \((x+2)(x-2) \geq d^2x^2\).
	Then we let \(c = d^2\).
	
	So now our goal is to solve \(x-2 \stackrel{?}{\geq} dx\).
	Rearranging yields \(x \stackrel{?}{\geq} \frac{2}{(1-d)}\).
	By letting \(d = \frac{1}{2}\), then we get \(x \geq 4\) and we can thus work backwards for the actual proof to find \(c = \frac{1}{4}\) with \(x_0 = 4\).
\end{example}

% todo -- limit definitions

\section{Analyzing Algorithms}

We can express algorithms in terms of functions. The function input is typically \(n\), the number of elements inputted into, or input size for, the algorithm, and the function output is typically time. Usually \textit{time} will be the number of certain operations performed during the algorithm.

\exsol{
	Come up with a function \(T(n)\) that describes the run-time of the following algorithm. Define run-time as the number of conditional evaluations. In the following algorithm, \(L\) is a list of integers of length \(n\).
	\begin{algorithmic}[1]
		\Function{GetMax}{\(L\), \(n\)\@}
			\State \(M \gets -\infty\)
			\State \(i \gets 0\)
			\While{\(i < n\)}
				\If{\(L[i] > M\)}
					\State \(M \gets L[i]\)
				\EndIf
				\State \(i \gets i+1\)
			\EndWhile
			\State \Return \(M\)
		\EndFunction
	\end{algorithmic}
}{
	We evaluate the if-statement in line 5 exactly \(n\) times, and we evaluate the while-loop condition exactly \(n+1\) times (the last time is when we break out of the loop). Thus, \(T(n) = n + n + 1 = 2n + 1\)
}

\exsol{
	Now come up with a function \(U(n)\) that outputs the number of \(\gets\) operations. Compare \(T(n)\) to \(U(n)\).
}{
	We have two initial stores from lines 1 and 2. Each while-loop iteration has a store command in line 8. The while-loop runs exactly \(n\) times, so this gives us \(n\) stores. Line 6 has a store command which is run \(\leq n\) times. We can model this using a natural number \(c \leq n\) where \(c\) is the number of times line 5 evaluates to true. Our function is thus \(U(n) = n + c + 2\)
	
	Since \(c \leq n\) we have that \(U(n) \leq T(n)\) for all \(n > 1\). Note that when \(n = 1\) we have \(T(n) = 3\) and \(U(n) = 4\) (since \(c = 1\)). In any case, we have that the growth-rates of both functions are solely dependent on \(n\) and are hence the same. Both functions are in \(\Theta(n)\)
}

\begin{defn}[The Loop-Heuristic]
	(algorithm analysis) For each nested loop in an algorithm, multiply \(n\) by the number of nested levels. This can be helpful in an initial analysis, however it will only take you so far.
\end{defn}

\begin{rem}
	We state the loop-heuristic as a definition, however it can be rephrased as a provable theorem.
\end{rem}

It is important to think about what is going on during the algorithm instead of blindly following the loop-heuristic. We present a few related examples.

\exsol{
	Analyze the following algorithm:
	\begin{algorithmic}[1]
		\Function{AnalyzeList}{\(L\), \(n\)\@}
			\State \(M \gets 0\)
			\For{\(i \gets 0\); \(i < n\); \(i \gets i + 1\)}
				\State \(k \gets L[i]\)
				\For{\(j \gets i\); \(j < n\); \(j \gets j + 1\)}
					\State \(k \gets k * L[j]\)
				\EndFor
				\State \(M \gets M + k\)
			\EndFor
			\State \Return \(M\)
		\EndFunction
	\end{algorithmic}
}{
	The algorithm contains 2 nested loops, so by the loop-heuristic our run-time is \(n^2\). We examine further. The outer loop goes through the list \(n\) times. For each iteration \(i\), we have a loop that goes through the list \(n - i\) times. The actual \textit{work} is done in the inner loop. Thus, the amount of work equals \(1 + 2 + 3 + \cdots + n = \frac{n(n+1)}{2} = \frac{1}{2}n^2 + \frac{1}{2}n\) by the Gaussian sum. This has a growth rate of \(n^2\), so we see that the loop-heuristic worked and the algorithm runs in \(\Oh(n^2)\). In fact, the amount of work does not change with respect to different input ordering, so the algorithm runs in \(\Theta(n^2)\)
}

\exsol{
	Examine the following algorithm:
	\begin{algorithmic}[1]
		\Function{AnalyzeGraph}{\(G = (V,E)\)\@}
			\State \(c \gets 0\)
			\For{\(v \in V\)}
				\For{\(w \in \text{UnvisitedNeighbors}(v)\)}
					\State Visit \(w\)
					\State \(c \gets c + 1\)
				\EndFor
			\EndFor
			\State \Return \(c\)
		\EndFunction
	\end{algorithmic}
}{
	\(G\) represents a graph, where \(V\) is the set of vertices and \(E\) is the set of edges. We usually denote \(|V| = n\) and \(|E| = m\).
	
	Let us first think about what the algorithm is \textit{doing}. We instantiate \(c=0\), then go through each unvisited vertex we increment \(c\). We consider the amount of work in the inner loop \(\Theta(1)\), and we do this exactly \(|V| = n\) times. So our algorithm runs in \(\Theta(V)\) (or \(\Theta(n)\)). % todo : undirected or directed? define neighbors?
}

\begin{rem}
	How would this change had we instead iterated through the \textit{neighbors}, instead of \textit{unvisited neighbors}? We will come back to this % todo
\end{rem}

Okay, so how does one determine whether an algorithm is \textit{good}? Well, it depends on how you define \textit{good}. Typically we define \textit{good} as having a low run-time. Unfortunately, an algorithm's run-time can change depending on the input. Remember: when writing algorithms, you cannot predetermine the input! This heeds way to the idea of \textit{best-case}, \textit{worst-case}, and \textit{average-case} analysis.

\begin{defn}[Best-case Analysis]
	Analyzing an algorithm based on an optimal input that minimizes its run-time.
\end{defn}

Best-case analysis is akin to bounding an algorithm's run-time function below, which is just determining the tightest Big-Omega bound. So, \(\Omega\) essentially tells you how fast the algorithm will run in the best case.

\begin{defn}[Worst-case Analysis]
	Analyzing an algorithm based on a pathological input that maximizes its run-time.
\end{defn}

By contrast, worst-case analysis is akin to bounding an algorithm's run-time function above, which is just determining the tightest Big-Oh bound. So, \(\Oh\) essentially tells you how fast the algorithm will run in the worst case.

\begin{defn}[Average-case Analysis]
	Analyzing an algorithm's run-time based on typical, expected, input.
\end{defn}

\begin{rem}
	You may be tempted to think that, as the analogy would imply, average-case analysis is akin to \(\Theta\). This is not true. \(\Theta\) tells you the \textit{exact} run-time, which is when \(\Oh\) and \(\Omega\) agree. If you know your algorithm run-time has a \(\Theta\) bound, then the average-case analysis will be this bound. The converse is not true though. If you have the average-case bound, this does not necessarily mean the worst-case or best-case is the same. An important example of this is the Quicksort algorithm.
\end{rem}

We present a pathological example to explain why these different analysis types are important.

\exsol{
	Examine the following algorithm:
	\begin{algorithmic}[1]
		\Function{Pathological}{\(L\), \(n\)\@}
		\If{\(n \equiv 0 \Mod{2}\)}
			\State 
		\Else
			\State \Return \(L[0]\)
		\EndIf
		\State \(c \gets 0\)
		\For{\(v \in V\)}
		\For{\(w \in \text{UnvisitedNeighbors}(v)\)}
		\State Visit \(w\)
		\State \(c \gets c + 1\)
		\EndFor
		\EndFor
		\State \Return \(c\)
		\EndFunction
	\end{algorithmic}
	% todo function that takes exponential on if-branch, and constant on else
}{
	
}

% maybe another that is less pathological

\subsection{Solving Summations}

% todo

% we did a similar thing during the number theory chapter.
% i guess make it more explicit here? idk
% we use the sum of c, i and i^2 here

% recall addition is linear, so blah blah blah

\subsection{Solving Recurrences}

% todo

% tree table descending example

\begin{thm}[The Master Theorem]
	% todo
\end{thm}

Proof omitted.

% another example, but use master thm

\section{Common Complexities}

There are a handful of common run-time complexities that you should be familiar with. In this book, we give them to you in ascending order of growth rate in Big Theta notation.

\begin{defn}[Constant]
	\[\Theta(1)\]
\end{defn}

\begin{defn}[Logarithmic]
	\[\Theta(\log n)\]
\end{defn}

\begin{defn}[Linear]
	\[\Theta(n)\]
\end{defn}

\begin{defn}[Polynomial]
	\[\Theta(n^c),\ c > 1\]
	We call \(\Theta(n^2)\) Quadratic
\end{defn}

\begin{defn}[Exponential]
	\[\Theta(c^n),\ c > 1\]
\end{defn}

\section{P vs NP}

There exist many YouTube videos discussing the famous P vs.\ NP problem (\href{https://www.youtube.com/watch?v=EHp4FPyajKQ}{ex 1}, \href{https://www.youtube.com/watch?v=YX40hbAHx3s}{ex 2}). Moreover, if you solve it, the Clay Mathematics Institute has \href{https://www.claymath.org/millennium-problems/p-vs-np-problem}{\$1,000,000} waiting for you. So what is it?

\begin{defn}[P]
	The class of computational problems that can be solved in \textbf{P}olynomial time.
\end{defn}

\begin{defn}[NP]
	The class of computational problems where a given solution can be \textit{verified} in polynomial time.
\end{defn}

\begin{rem}
	NP stands for \textit{Nondeterministic Polynomial}. It does \textbf{not} stand for \textit{non-polynomial} as some people think.
\end{rem}



\begin{defn}[Reductions]
	
\end{defn}

\begin{defn}[NP-Complete]
	A computational problem is NP-Complete if it satisfies two conditions:
	\begin{enumerate}
		\item The problem is in NP
		\item Every problem in NP is reducible to it in polynomial time
	\end{enumerate}
\end{defn}

\begin{rem}
	Problems that satisfy the second condition above, and not necessarily the first, are called \textbf{NP-Hard}.
\end{rem}

The second condition is tricky.
As of now, the set of NP-Complete problems is empty.
So we need some form of \textit{starter problem} to which all reductions can be made.

\begin{thm}[The Cook-Levin Theorem]
	The Boolean satisfiability problem is NP-Complete.
\end{thm}

\begin{rem}
	While at this point in the course you likely can understand the proof, we omit it for scope.
\end{rem}

We leave this discussion with one final remark.
\(\text{P}=\text{NP}\) will render most (all) cryptographic algorithms and services broken.
This is because most modern cryptography is built on the observation that factoring very large primes is \textit{hard}.
If you are interested, look into RSA and/or the Diffie-Hellman key exchange.

\section{The Halting Problem}

Infinite loops are a software developer's worst nightmare. They are hard to find, debug, and (sometimes) fix. If only there were a piece of software that could examine your code and tell you whether there are any infinite loops. More abstractly, if it could tell you whether your program continues forever or eventually stops.

Well, we are sorry to break the news to you, but this piece of software can never exist; sort-of.

The halting problem is most commonly used as an introduction to computability theory. What problems can we compute? What does it even mean to be computable? These questions are out of scope of this book, but if the following problem interests you, consider taking a course on theory of computation.

\begin{defn}[The Halting Problem]
	A computational problem of determining whether an arbitrary computer program, with input, will stop or run forever.
\end{defn}

Think to yourself whether you can come up with a solution program. It is a difficult problem. Alan Turing proved in 1936 that no such algorithm could exist \textit{for all} pairs of programs and inputs. This proof led to (created) the Turing machine, and arguably all of modern computer science. The proof sketch follows. We call it a sketch because the actual proof is significantly more in-depth.

\begin{proof}
	Assume that such an algorithm (code description, function, method, whatever you want to call it) \(h\) exists. The function \(h : P \times I \rightarrow \mathbb{B}\) given by \(h : (p,i) \mapsto b\) tells us whether a given input program \(p \in P\) (the set of all programs) with input \(i \in I\) (the set of all program inputs) will halt (stop) or not: \(b \in \mathbb{B} = \{\text{True}, \text{False}\}\).
	
	Then the following procedure \(\textsc{g} \in P\):
	\begin{algorithmic}[1]
		\Function{g}{\(\cdot\)\@}
			\If{\(h(\textsc{g},\cdot)\)}
				\While{True}
				\Comment{loop forever}
				\EndWhile
			\EndIf
		\EndFunction
	\end{algorithmic}
	
	We have two cases: \(h(\textsc{g},\cdot)\) returns True, or \(h(\textsc{g},\cdot)\) returns False.
	
	Case 1: \(h(\textsc{g},\cdot) = \text{True}\). Then \textsc{g} stops. But since \(h\) returned True, the body of the if-statement will be executed. Then \textsc{g} will loop forever. But this contradicts the output of \(h\).
	
	Case 2: \(h(\textsc{g},\cdot) = \text{False}\). Then \textsc{g} does not stop. But since \(h\) returned False, the body of the if-statement is \textit{not} executed, so \textsc{g} will stop. But this contradicts the output of \(h\).
	
	Thus, the halting problem is undecidable (impossible to answer for all inputs).
\end{proof}

% todo

\section{Summary}

\begin{itemize}
	\item 
	\item 
	\item 
\end{itemize}

\section{Practice}

\begin{enumerate}
	\item Analyze the following algorithm:
	\begin{algorithmic}[1]
		\Function{SomeFunc}{\(n\)\@}
			\State something
		\EndFunction
	\end{algorithmic}
	\item Order the following complexities: \(\Theta(n \log n)\), \(\Theta(n \log (n^2))\), \(\Theta(n)\), \(\Theta(n^2)\). Explain why your ordering is correct.
	\item Order the following complexities: \(\Theta(\log (3n))\), \(\Theta(n^n)\), \(\Theta(n!)\), \(\Theta(\log (\log n))\).
\end{enumerate}

%\section{Solutions}
%
%\begin{enumerate}
%	\item 
%	\item Since \(n \log (n^2) = 2n \log n\) we have that \(n \log (n^2) \in \Theta(n \log n)\). So the growth-rate of those two functions are the same. It suffices to order \(\Theta(n \log n)\), \(\Theta(n)\), \(\Theta(n^2)\). We know \(n \in \Oh(n^2)\). It should be clear that \(n \in \Oh(n \log n)\) (consider \(1 < \log_b n \Rightarrow n < n \log_b n\) for all \(n > b\)). Similar logic can be applied to show that \(n \log n \in \Oh(n^2)\). Thus our ordering is \(n < n \log n < n \log (n^2) < n^2\) where \(a <b \) is shorthand for \(a \in \Oh(b)\)
%	\item \(\log \log n < \log (3n) < n! < n^n\) where \(a <b \) is shorthand for \(a \in \Oh(b)\)
%\end{enumerate}
\end{document}
